{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "RMToLbe8x7lF",
    "outputId": "e6b07baa-ccbc-4bc1-da2d-49f7a1492c0c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first neural network with keras tutorial\n",
    "from numpy import loadtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from google.colab import files\n",
    "import pandas as pd\n",
    "#Scaling data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": "OK"
      }
     }
    },
    "colab_type": "code",
    "id": "cC11AH6_zSmv",
    "outputId": "2a7c8573-7a72-4424-8f96-5e8df81adfee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-9da8cb8a-edc7-4261-ae12-627e2c4ffc73\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-9da8cb8a-edc7-4261-ae12-627e2c4ffc73\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving heart.csv to heart.csv\n"
     ]
    }
   ],
   "source": [
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nlh5S5Gu0L22",
    "outputId": "0c4d8475-f04b-4cf6-8f68-197f5a9712c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User uploaded file \"heart.csv\" with length 11328 bytes\n"
     ]
    }
   ],
   "source": [
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgTUkdW04B7N"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "df = pd.read_csv(io.StringIO(uploaded['heart.csv'].decode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "VY5ehPJX4v1s",
    "outputId": "95834106-d5a1-45cd-f0bf-40e7e63e3a95"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  ...  exang  oldpeak  slope  ca  thal  target\n",
       "0   63    1   3       145   233    1  ...      0      2.3      0   0     1       1\n",
       "1   37    1   2       130   250    0  ...      0      3.5      0   0     2       1\n",
       "2   41    0   1       130   204    0  ...      0      1.4      2   0     2       1\n",
       "3   56    1   1       120   236    0  ...      0      0.8      2   0     2       1\n",
       "4   57    0   0       120   354    0  ...      1      0.6      2   0     2       1\n",
       "\n",
       "[5 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apmd-6QMaEKw"
   },
   "outputs": [],
   "source": [
    "#Since 'cp', 'thal', 'sex' and 'slope' are categorical variables we'll turn them into dummy variables.\n",
    "df_cat = pd.get_dummies(df,columns = ['sex','cp','thal','slope'])\n",
    "df=pd.DataFrame(df_cat, columns = df_cat.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZhhVRbLaJqO"
   },
   "outputs": [],
   "source": [
    "#Scaling Dataset\n",
    "mms = MinMaxScaler()\n",
    "df_scaled = mms.fit_transform(df)\n",
    "df = pd.DataFrame(df_scaled, columns = df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "colab_type": "code",
    "id": "5ByjFDdPatL9",
    "outputId": "2e5439ba-8ae0-4664-df47-5033390d9657"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>ca</th>\n",
       "      <th>target</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>sex_1</th>\n",
       "      <th>cp_0</th>\n",
       "      <th>cp_1</th>\n",
       "      <th>cp_2</th>\n",
       "      <th>cp_3</th>\n",
       "      <th>thal_0</th>\n",
       "      <th>thal_1</th>\n",
       "      <th>thal_2</th>\n",
       "      <th>thal_3</th>\n",
       "      <th>slope_0</th>\n",
       "      <th>slope_1</th>\n",
       "      <th>slope_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  trestbps      chol  fbs  ...  thal_3  slope_0  slope_1  slope_2\n",
       "0  0.708333  0.481132  0.244292  1.0  ...     0.0      1.0      0.0      0.0\n",
       "1  0.166667  0.339623  0.283105  0.0  ...     0.0      1.0      0.0      0.0\n",
       "2  0.250000  0.339623  0.178082  0.0  ...     0.0      0.0      0.0      1.0\n",
       "3  0.562500  0.245283  0.251142  0.0  ...     0.0      0.0      0.0      1.0\n",
       "4  0.583333  0.245283  0.520548  0.0  ...     0.0      0.0      0.0      1.0\n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgrF6Op-BiGi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qq9mA3b4yWH"
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = df.drop(columns='target')\n",
    "Y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6laC3idiBpoO"
   },
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2,random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3fhJ_AOLByQG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "colab_type": "code",
    "id": "-B3UR1XC5ToM",
    "outputId": "61c4d296-c525-4002-95a5-eb5421a488f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(18, input_dim=22, activation='relu'))\n",
    "model.add(Dense(22, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "colab_type": "code",
    "id": "LTZK8GojDWt_",
    "outputId": "54244594-e349-43f1-c1f4-151d1c98fa82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hNyqkjOnDkMA",
    "outputId": "d7468c30-e7db-47d8-bf47-0b03518b5038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "242/242 [==============================] - 0s 98us/step - loss: 0.0889 - acc: 0.9793\n",
      "Epoch 2/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0857 - acc: 0.9793\n",
      "Epoch 3/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0832 - acc: 0.9835\n",
      "Epoch 4/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0829 - acc: 0.9835\n",
      "Epoch 5/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0853 - acc: 0.9835\n",
      "Epoch 6/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0877 - acc: 0.9793\n",
      "Epoch 7/300\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.0875 - acc: 0.9876\n",
      "Epoch 8/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0851 - acc: 0.9835\n",
      "Epoch 9/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0886 - acc: 0.9711\n",
      "Epoch 10/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0828 - acc: 0.9793\n",
      "Epoch 11/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0814 - acc: 0.9835\n",
      "Epoch 12/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0806 - acc: 0.9835\n",
      "Epoch 13/300\n",
      "242/242 [==============================] - 0s 111us/step - loss: 0.0840 - acc: 0.9917\n",
      "Epoch 14/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0805 - acc: 0.9876\n",
      "Epoch 15/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0817 - acc: 0.9835\n",
      "Epoch 16/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0800 - acc: 0.9876\n",
      "Epoch 17/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0834 - acc: 0.9793\n",
      "Epoch 18/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0803 - acc: 0.9835\n",
      "Epoch 19/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0738 - acc: 0.9917\n",
      "Epoch 20/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0776 - acc: 0.9835\n",
      "Epoch 21/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0742 - acc: 0.9917\n",
      "Epoch 22/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0753 - acc: 0.9876\n",
      "Epoch 23/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0735 - acc: 0.9917\n",
      "Epoch 24/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0756 - acc: 0.9835\n",
      "Epoch 25/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0742 - acc: 0.9835\n",
      "Epoch 26/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0725 - acc: 0.9876\n",
      "Epoch 27/300\n",
      "242/242 [==============================] - 0s 98us/step - loss: 0.0715 - acc: 0.9876\n",
      "Epoch 28/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0702 - acc: 0.9876\n",
      "Epoch 29/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0701 - acc: 0.9876\n",
      "Epoch 30/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0809 - acc: 0.9711\n",
      "Epoch 31/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0707 - acc: 0.9835\n",
      "Epoch 32/300\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.0715 - acc: 0.9835\n",
      "Epoch 33/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0731 - acc: 0.9917\n",
      "Epoch 34/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0730 - acc: 0.9793\n",
      "Epoch 35/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0673 - acc: 0.9876\n",
      "Epoch 36/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0722 - acc: 0.9835\n",
      "Epoch 37/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0681 - acc: 0.9917\n",
      "Epoch 38/300\n",
      "242/242 [==============================] - 0s 107us/step - loss: 0.0694 - acc: 0.9917\n",
      "Epoch 39/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0720 - acc: 0.9876\n",
      "Epoch 40/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0726 - acc: 0.9835\n",
      "Epoch 41/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0673 - acc: 0.9876\n",
      "Epoch 42/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0658 - acc: 0.9876\n",
      "Epoch 43/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0655 - acc: 0.9876\n",
      "Epoch 44/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0662 - acc: 0.9835\n",
      "Epoch 45/300\n",
      "242/242 [==============================] - 0s 96us/step - loss: 0.0638 - acc: 0.9876\n",
      "Epoch 46/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0654 - acc: 0.9876\n",
      "Epoch 47/300\n",
      "242/242 [==============================] - 0s 102us/step - loss: 0.0643 - acc: 0.9876\n",
      "Epoch 48/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0638 - acc: 0.9876\n",
      "Epoch 49/300\n",
      "242/242 [==============================] - 0s 145us/step - loss: 0.0625 - acc: 0.9876\n",
      "Epoch 50/300\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.0620 - acc: 0.9917\n",
      "Epoch 51/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0619 - acc: 0.9917\n",
      "Epoch 52/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0607 - acc: 0.9917\n",
      "Epoch 53/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0624 - acc: 0.9917\n",
      "Epoch 54/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0643 - acc: 0.9959\n",
      "Epoch 55/300\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.0632 - acc: 0.9959\n",
      "Epoch 56/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0598 - acc: 0.9917\n",
      "Epoch 57/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0599 - acc: 0.9917\n",
      "Epoch 58/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0607 - acc: 0.9835\n",
      "Epoch 59/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0705 - acc: 0.9917\n",
      "Epoch 60/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0674 - acc: 0.9959\n",
      "Epoch 61/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0642 - acc: 0.9876\n",
      "Epoch 62/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0625 - acc: 0.9876\n",
      "Epoch 63/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0609 - acc: 0.9876\n",
      "Epoch 64/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0586 - acc: 0.9876\n",
      "Epoch 65/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0582 - acc: 0.9876\n",
      "Epoch 66/300\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.0567 - acc: 0.9876\n",
      "Epoch 67/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0563 - acc: 0.9876\n",
      "Epoch 68/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0555 - acc: 0.9835\n",
      "Epoch 69/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0554 - acc: 0.9917\n",
      "Epoch 70/300\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.0557 - acc: 0.9959\n",
      "Epoch 71/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0630 - acc: 0.9793\n",
      "Epoch 72/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0579 - acc: 0.9959\n",
      "Epoch 73/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0667 - acc: 0.9793\n",
      "Epoch 74/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0655 - acc: 0.9876\n",
      "Epoch 75/300\n",
      "242/242 [==============================] - 0s 96us/step - loss: 0.0605 - acc: 0.9876\n",
      "Epoch 76/300\n",
      "242/242 [==============================] - 0s 102us/step - loss: 0.0565 - acc: 0.9917\n",
      "Epoch 77/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0524 - acc: 0.9959\n",
      "Epoch 78/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0518 - acc: 0.9959\n",
      "Epoch 79/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0560 - acc: 0.9959\n",
      "Epoch 80/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0599 - acc: 0.9876\n",
      "Epoch 81/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0596 - acc: 0.9917\n",
      "Epoch 82/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0526 - acc: 0.9917\n",
      "Epoch 83/300\n",
      "242/242 [==============================] - 0s 98us/step - loss: 0.0525 - acc: 0.9959\n",
      "Epoch 84/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0623 - acc: 0.9835\n",
      "Epoch 85/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0505 - acc: 0.9959\n",
      "Epoch 86/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0523 - acc: 0.9917\n",
      "Epoch 87/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0545 - acc: 0.9917\n",
      "Epoch 88/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0507 - acc: 0.9959\n",
      "Epoch 89/300\n",
      "242/242 [==============================] - 0s 110us/step - loss: 0.0482 - acc: 0.9959\n",
      "Epoch 90/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0535 - acc: 0.9917\n",
      "Epoch 91/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0502 - acc: 0.9959\n",
      "Epoch 92/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0479 - acc: 0.9959\n",
      "Epoch 93/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0531 - acc: 0.9917\n",
      "Epoch 94/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0490 - acc: 0.9959\n",
      "Epoch 95/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0495 - acc: 0.9959\n",
      "Epoch 96/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0479 - acc: 0.9959\n",
      "Epoch 97/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0467 - acc: 0.9959\n",
      "Epoch 98/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0460 - acc: 0.9959\n",
      "Epoch 99/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0455 - acc: 0.9959\n",
      "Epoch 100/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0450 - acc: 0.9959\n",
      "Epoch 101/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0476 - acc: 0.9876\n",
      "Epoch 102/300\n",
      "242/242 [==============================] - 0s 122us/step - loss: 0.0485 - acc: 0.9917\n",
      "Epoch 103/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0483 - acc: 0.9959\n",
      "Epoch 104/300\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.0449 - acc: 0.9959\n",
      "Epoch 105/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0444 - acc: 0.9917\n",
      "Epoch 106/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0433 - acc: 0.9959\n",
      "Epoch 107/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0514 - acc: 0.9876\n",
      "Epoch 108/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0490 - acc: 0.9917\n",
      "Epoch 109/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0456 - acc: 0.9876\n",
      "Epoch 110/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0459 - acc: 0.9876\n",
      "Epoch 111/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0455 - acc: 0.9876\n",
      "Epoch 112/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0421 - acc: 0.9917\n",
      "Epoch 113/300\n",
      "242/242 [==============================] - 0s 102us/step - loss: 0.0420 - acc: 0.9917\n",
      "Epoch 114/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0416 - acc: 0.9917\n",
      "Epoch 115/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0420 - acc: 0.9959\n",
      "Epoch 116/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0412 - acc: 0.9959\n",
      "Epoch 117/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0403 - acc: 0.9959\n",
      "Epoch 118/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0416 - acc: 0.9959\n",
      "Epoch 119/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0400 - acc: 0.9959\n",
      "Epoch 120/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0398 - acc: 0.9959\n",
      "Epoch 121/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0408 - acc: 0.9959\n",
      "Epoch 122/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0394 - acc: 0.9959\n",
      "Epoch 123/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0397 - acc: 0.9959\n",
      "Epoch 124/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0396 - acc: 0.9959\n",
      "Epoch 125/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0421 - acc: 0.9917\n",
      "Epoch 126/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0418 - acc: 1.0000\n",
      "Epoch 127/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0438 - acc: 0.9959\n",
      "Epoch 128/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0420 - acc: 0.9959\n",
      "Epoch 129/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0413 - acc: 1.0000\n",
      "Epoch 130/300\n",
      "242/242 [==============================] - 0s 125us/step - loss: 0.0395 - acc: 1.0000\n",
      "Epoch 131/300\n",
      "242/242 [==============================] - 0s 115us/step - loss: 0.0388 - acc: 1.0000\n",
      "Epoch 132/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 133/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0375 - acc: 0.9959\n",
      "Epoch 134/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0375 - acc: 0.9959\n",
      "Epoch 135/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0368 - acc: 1.0000\n",
      "Epoch 136/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0377 - acc: 0.9959\n",
      "Epoch 137/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0367 - acc: 0.9959\n",
      "Epoch 138/300\n",
      "242/242 [==============================] - 0s 133us/step - loss: 0.0360 - acc: 1.0000\n",
      "Epoch 139/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0355 - acc: 0.9959\n",
      "Epoch 140/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0353 - acc: 1.0000\n",
      "Epoch 141/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0370 - acc: 1.0000\n",
      "Epoch 142/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0351 - acc: 1.0000\n",
      "Epoch 143/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0347 - acc: 0.9959\n",
      "Epoch 144/300\n",
      "242/242 [==============================] - 0s 110us/step - loss: 0.0346 - acc: 1.0000\n",
      "Epoch 145/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0337 - acc: 0.9959\n",
      "Epoch 146/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0341 - acc: 0.9959\n",
      "Epoch 147/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0334 - acc: 0.9959\n",
      "Epoch 148/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0355 - acc: 0.9959\n",
      "Epoch 149/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0338 - acc: 0.9959\n",
      "Epoch 150/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0339 - acc: 0.9959\n",
      "Epoch 151/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0331 - acc: 1.0000\n",
      "Epoch 152/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 153/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0331 - acc: 0.9959\n",
      "Epoch 154/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0324 - acc: 1.0000\n",
      "Epoch 155/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 156/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0325 - acc: 1.0000\n",
      "Epoch 157/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0322 - acc: 0.9959\n",
      "Epoch 158/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0326 - acc: 1.0000\n",
      "Epoch 159/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0310 - acc: 1.0000\n",
      "Epoch 160/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0310 - acc: 1.0000\n",
      "Epoch 161/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0312 - acc: 1.0000\n",
      "Epoch 162/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0358 - acc: 0.9917\n",
      "Epoch 163/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0359 - acc: 0.9959\n",
      "Epoch 164/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0321 - acc: 0.9959\n",
      "Epoch 165/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0307 - acc: 1.0000\n",
      "Epoch 166/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0304 - acc: 1.0000\n",
      "Epoch 167/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 168/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0300 - acc: 1.0000\n",
      "Epoch 169/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0297 - acc: 1.0000\n",
      "Epoch 170/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0291 - acc: 1.0000\n",
      "Epoch 171/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 172/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0290 - acc: 1.0000\n",
      "Epoch 173/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 174/300\n",
      "242/242 [==============================] - 0s 108us/step - loss: 0.0304 - acc: 1.0000\n",
      "Epoch 175/300\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.0289 - acc: 1.0000\n",
      "Epoch 176/300\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.0294 - acc: 0.9959\n",
      "Epoch 177/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0335 - acc: 0.9959\n",
      "Epoch 178/300\n",
      "242/242 [==============================] - 0s 100us/step - loss: 0.0292 - acc: 0.9959\n",
      "Epoch 179/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0299 - acc: 0.9959\n",
      "Epoch 180/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0277 - acc: 1.0000\n",
      "Epoch 181/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0274 - acc: 1.0000\n",
      "Epoch 182/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0289 - acc: 0.9959\n",
      "Epoch 183/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0290 - acc: 0.9959\n",
      "Epoch 184/300\n",
      "242/242 [==============================] - 0s 99us/step - loss: 0.0281 - acc: 0.9959\n",
      "Epoch 185/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0277 - acc: 0.9959\n",
      "Epoch 186/300\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.0275 - acc: 0.9959\n",
      "Epoch 187/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0269 - acc: 1.0000\n",
      "Epoch 188/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0271 - acc: 1.0000\n",
      "Epoch 189/300\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.0258 - acc: 1.0000\n",
      "Epoch 190/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0257 - acc: 1.0000\n",
      "Epoch 191/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0279 - acc: 0.9959\n",
      "Epoch 192/300\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.0267 - acc: 1.0000\n",
      "Epoch 193/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0276 - acc: 1.0000\n",
      "Epoch 194/300\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.0252 - acc: 1.0000\n",
      "Epoch 195/300\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 196/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0261 - acc: 1.0000\n",
      "Epoch 197/300\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 198/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0284 - acc: 1.0000\n",
      "Epoch 199/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0293 - acc: 1.0000\n",
      "Epoch 200/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 201/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0248 - acc: 1.0000\n",
      "Epoch 202/300\n",
      "242/242 [==============================] - 0s 67us/step - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 203/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0260 - acc: 1.0000\n",
      "Epoch 204/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0244 - acc: 1.0000\n",
      "Epoch 205/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0245 - acc: 1.0000\n",
      "Epoch 206/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0241 - acc: 1.0000\n",
      "Epoch 207/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0239 - acc: 1.0000\n",
      "Epoch 208/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0262 - acc: 1.0000\n",
      "Epoch 209/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0232 - acc: 1.0000\n",
      "Epoch 210/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0227 - acc: 1.0000\n",
      "Epoch 211/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 212/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0233 - acc: 1.0000\n",
      "Epoch 213/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0249 - acc: 0.9959\n",
      "Epoch 214/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0285 - acc: 0.9959\n",
      "Epoch 215/300\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.0252 - acc: 0.9959\n",
      "Epoch 216/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0231 - acc: 0.9959\n",
      "Epoch 217/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0249 - acc: 0.9959\n",
      "Epoch 218/300\n",
      "242/242 [==============================] - 0s 97us/step - loss: 0.0239 - acc: 1.0000\n",
      "Epoch 219/300\n",
      "242/242 [==============================] - 0s 100us/step - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 220/300\n",
      "242/242 [==============================] - 0s 130us/step - loss: 0.0230 - acc: 1.0000\n",
      "Epoch 221/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0217 - acc: 1.0000\n",
      "Epoch 222/300\n",
      "242/242 [==============================] - 0s 100us/step - loss: 0.0215 - acc: 1.0000\n",
      "Epoch 223/300\n",
      "242/242 [==============================] - 0s 96us/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 224/300\n",
      "242/242 [==============================] - 0s 100us/step - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 225/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 226/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0204 - acc: 1.0000\n",
      "Epoch 227/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0205 - acc: 1.0000\n",
      "Epoch 228/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0211 - acc: 1.0000\n",
      "Epoch 229/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0225 - acc: 0.9959\n",
      "Epoch 230/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0227 - acc: 0.9959\n",
      "Epoch 231/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0238 - acc: 1.0000\n",
      "Epoch 232/300\n",
      "242/242 [==============================] - 0s 92us/step - loss: 0.0244 - acc: 0.9959\n",
      "Epoch 233/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 234/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 235/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0195 - acc: 1.0000\n",
      "Epoch 236/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 237/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0201 - acc: 1.0000\n",
      "Epoch 238/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0194 - acc: 1.0000\n",
      "Epoch 239/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0190 - acc: 1.0000\n",
      "Epoch 240/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 241/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0203 - acc: 1.0000\n",
      "Epoch 242/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0209 - acc: 1.0000\n",
      "Epoch 243/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0208 - acc: 1.0000\n",
      "Epoch 244/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0188 - acc: 1.0000\n",
      "Epoch 245/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 246/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0189 - acc: 1.0000\n",
      "Epoch 247/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0196 - acc: 1.0000\n",
      "Epoch 248/300\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 249/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0184 - acc: 1.0000\n",
      "Epoch 250/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0202 - acc: 1.0000\n",
      "Epoch 251/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0198 - acc: 0.9959\n",
      "Epoch 252/300\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 253/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0211 - acc: 0.9959\n",
      "Epoch 254/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0246 - acc: 0.9959\n",
      "Epoch 255/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0203 - acc: 0.9959\n",
      "Epoch 256/300\n",
      "242/242 [==============================] - 0s 81us/step - loss: 0.0199 - acc: 0.9959\n",
      "Epoch 257/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0182 - acc: 1.0000\n",
      "Epoch 258/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0244 - acc: 0.9959\n",
      "Epoch 259/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0230 - acc: 0.9959\n",
      "Epoch 260/300\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.0198 - acc: 0.9959\n",
      "Epoch 261/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0180 - acc: 0.9959\n",
      "Epoch 262/300\n",
      "242/242 [==============================] - 0s 74us/step - loss: 0.0168 - acc: 1.0000\n",
      "Epoch 263/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0173 - acc: 1.0000\n",
      "Epoch 264/300\n",
      "242/242 [==============================] - 0s 71us/step - loss: 0.0177 - acc: 1.0000\n",
      "Epoch 265/300\n",
      "242/242 [==============================] - 0s 70us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 266/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 267/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0161 - acc: 1.0000\n",
      "Epoch 268/300\n",
      "242/242 [==============================] - 0s 77us/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 269/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0164 - acc: 1.0000\n",
      "Epoch 270/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 271/300\n",
      "242/242 [==============================] - 0s 83us/step - loss: 0.0160 - acc: 1.0000\n",
      "Epoch 272/300\n",
      "242/242 [==============================] - 0s 87us/step - loss: 0.0162 - acc: 1.0000\n",
      "Epoch 273/300\n",
      "242/242 [==============================] - 0s 90us/step - loss: 0.0153 - acc: 1.0000\n",
      "Epoch 274/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0237 - acc: 0.9959\n",
      "Epoch 275/300\n",
      "242/242 [==============================] - 0s 88us/step - loss: 0.0215 - acc: 0.9959\n",
      "Epoch 276/300\n",
      "242/242 [==============================] - 0s 104us/step - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 277/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 278/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0150 - acc: 1.0000\n",
      "Epoch 279/300\n",
      "242/242 [==============================] - 0s 93us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 280/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0147 - acc: 1.0000\n",
      "Epoch 281/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 282/300\n",
      "242/242 [==============================] - 0s 75us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 283/300\n",
      "242/242 [==============================] - 0s 76us/step - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 284/300\n",
      "242/242 [==============================] - 0s 103us/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 285/300\n",
      "242/242 [==============================] - 0s 86us/step - loss: 0.0210 - acc: 0.9959\n",
      "Epoch 286/300\n",
      "242/242 [==============================] - 0s 94us/step - loss: 0.0191 - acc: 0.9959\n",
      "Epoch 287/300\n",
      "242/242 [==============================] - 0s 78us/step - loss: 0.0154 - acc: 1.0000\n",
      "Epoch 288/300\n",
      "242/242 [==============================] - 0s 80us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 289/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 290/300\n",
      "242/242 [==============================] - 0s 89us/step - loss: 0.0146 - acc: 1.0000\n",
      "Epoch 291/300\n",
      "242/242 [==============================] - 0s 69us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 292/300\n",
      "242/242 [==============================] - 0s 79us/step - loss: 0.0141 - acc: 1.0000\n",
      "Epoch 293/300\n",
      "242/242 [==============================] - 0s 84us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 294/300\n",
      "242/242 [==============================] - 0s 85us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 295/300\n",
      "242/242 [==============================] - 0s 82us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 296/300\n",
      "242/242 [==============================] - 0s 72us/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 297/300\n",
      "242/242 [==============================] - 0s 111us/step - loss: 0.0136 - acc: 1.0000\n",
      "Epoch 298/300\n",
      "242/242 [==============================] - 0s 91us/step - loss: 0.0131 - acc: 1.0000\n",
      "Epoch 299/300\n",
      "242/242 [==============================] - 0s 95us/step - loss: 0.0138 - acc: 1.0000\n",
      "Epoch 300/300\n",
      "242/242 [==============================] - 0s 101us/step - loss: 0.0133 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f24171577b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(xtrain, ytrain, epochs=300, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "H3T2AORqDuDO",
    "outputId": "14de79c8-38c0-488b-e66e-9c72e7e9df02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242/242 [==============================] - 0s 43us/step\n",
      "Training Accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(xtrain, ytrain)\n",
    "print('Training Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "476yO_5oD7_t"
   },
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions = model.predict_classes(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "821PpuzbEMnz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "_acc = accuracy_score(ytest,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3Dd_FU4XYIj2",
    "outputId": "26f3adc7-3069-4a0b-8f69-13160cc9b1d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.16\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy: %.2f' % (_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "colab_type": "code",
    "id": "gVnE-23RJn-D",
    "outputId": "fe669cdc-32ab-45ee-f10b-760df9263477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 18)                414       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 22)                418       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 23        \n",
      "=================================================================\n",
      "Total params: 855\n",
      "Trainable params: 855\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RbMFL7k9LQS8"
   },
   "outputs": [],
   "source": [
    "#Saving Model\n",
    "model.save('DeepLearningModelForHeart.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ywv4PfFMJLH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0oP_UpsMYm-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A7ODGNtM6Pq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIr5Jd-ANB5n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lMU8K3DWNKjl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZDJmreiNNdC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eql1GcQ2N38K"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HeartUsingDeepLearning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
