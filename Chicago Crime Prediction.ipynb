{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present, minus the most recent seven days. Data is extracted from the Chicago Police Department's CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims, addresses are shown at the block level only and specific locations are not identified. Should you have questions about this dataset, you may contact the Research & Development Division of the Chicago Police Department at 312.745.6071 or RDAnalysis@chicagopolice.org. Disclaimer: These crimes may be based upon preliminary information supplied to the Police Department by the reporting parties that have not been verified. The preliminary crime classifications may be changed at a later date based upon additional investigation and there is always the possibility of mechanical or human error. Therefore, the Chicago Police Department does not guarantee (either expressed or implied) the accuracy, completeness, timeliness, or correct sequencing of the information and the information should not be used for comparison purposes over time. The Chicago Police Department will not be responsible for any error or omission, or for the use of, or the results obtained from the use of this information. All data visualizations on maps should be considered approximate and attempts to derive specific addresses are strictly prohibited. The Chicago Police Department is not responsible for the content of any off-site pages that are referenced by or that reference this web page other than an official City of Chicago or Chicago Police Department web page. The user specifically acknowledges that the Chicago Police Department is not responsible for any defamatory, offensive, misleading, or illegal conduct of other users, links, or third parties and that the risk of injury from the foregoing rests entirely with the user. The unauthorized use of the words \"Chicago Police Department,\" \"Chicago Police,\" or any colorable imitation of these words or the unauthorized use of the Chicago Police Department logo is unlawful. This web page does not, in any way, authorize such use. Data are updated daily. The dataset contains more than 6,000,000 records/rows of data and cannot be viewed in full in Microsoft Excel. To access a list of Chicago Police Department - Illinois Uniform Crime Reporting (IUCR) codes, go to http://data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e\n",
    "\n",
    "Content\n",
    "ID - Unique identifier for the record.\n",
    "\n",
    "Case Number - The Chicago Police Department RD Number (Records Division Number), which is unique to the incident.\n",
    "\n",
    "Date - Date when the incident occurred. this is sometimes a best estimate.\n",
    "\n",
    "Block - The partially redacted address where the incident occurred, placing it on the same block as the actual address.\n",
    "\n",
    "IUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https://data.cityofchicago.org/d/c7ck-438e.\n",
    "\n",
    "Primary Type - The primary description of the IUCR code.\n",
    "\n",
    "Description - The secondary description of the IUCR code, a subcategory of the primary description.\n",
    "\n",
    "Location Description - Description of the location where the incident occurred.\n",
    "\n",
    "Arrest - Indicates whether an arrest was made.\n",
    "\n",
    "Domestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act.\n",
    "\n",
    "Beat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area â€“ each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https://data.cityofchicago.org/d/aerh-rz74.\n",
    "\n",
    "District - Indicates the police district where the incident occurred. See the districts at https://data.cityofchicago.org/d/fthy-xz3r.\n",
    "\n",
    "Ward - The ward (City Council district) where the incident occurred. See the wards at https://data.cityofchicago.org/d/sp34-6z76.\n",
    "\n",
    "Community Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https://data.cityofchicago.org/d/cauq-8yn6.\n",
    "\n",
    "FBI Code - Indicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http://gis.chicagopolice.org/clearmap_crime_sums/crime_types.html.\n",
    "\n",
    "X Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
    "\n",
    "Y Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
    "\n",
    "Year - Year the incident occurred.\n",
    "\n",
    "Updated On - Date and time the record was last updated.\n",
    "\n",
    "Latitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
    "\n",
    "Longitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block.\n",
    "\n",
    "Location - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing in project \n",
    "Data Cleanup\n",
    "Data Visualization\n",
    "Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.preprocessing \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Importing required library module\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# scale the features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"poster\")\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset/Crimes_-_2001_to_present.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(' ',np.NaN,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[(df['Year'] >= 2011) & (df['Year'] <= 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Primary Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location Description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot these for better visualization\n",
    "crime_type_df = df['Primary Type'].value_counts(ascending=True)\n",
    "\n",
    "## Some formatting to make it look nicer\n",
    "fig=plt.figure(figsize=(18, 16))\n",
    "plt.title(\"Frequency of Crimes Per Crime Type\")\n",
    "plt.xlabel(\"Frequency of Crimes\")\n",
    "plt.ylabel(\"Type of Crime\")\n",
    "ax = crime_type_df.plot(kind='barh')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count number of reported crimes for each year\n",
    "df['Year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot these for better visualization\n",
    "crime_year_df = df['Year'].value_counts(ascending=True)\n",
    "\n",
    "## Some formatting to make it look nicer\n",
    "fig=plt.figure(figsize=(10, 8))\n",
    "plt.title(\"Frequency of Crimes Per Year in Chicago\")\n",
    "plt.xlabel(\"Frequency of Crimes\")\n",
    "plt.ylabel(\"Year\")\n",
    "ax = crime_year_df.plot(kind='barh')\n",
    "ax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check if any rows are missing data and are null\n",
    "df['Arrest'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count number of successful arrests for each year\n",
    "df['Arrest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Arrest', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write code here\n",
    "sns.boxplot(x='Arrest',y='Year',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert values into percentages\n",
    "arrest_df = df['Arrest'].value_counts()\n",
    "arrest_percent = (arrest_df / df['Arrest'].sum()) * 100 \n",
    "\n",
    "## Rename Series.name\n",
    "arrest_percent.rename(\"% of Arrests\",inplace=True)\n",
    "\n",
    "## Rename True and False to % Arrested and % Not Arrested\n",
    "arrest_percent.rename({True: '% Arrested', False: '% Not Arrested'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format pie chart to nicely show percentage and count\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%  ({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "## Plot results in a pie chart\n",
    "arrest_percent.plot.pie(fontsize=11,\n",
    "                       autopct=make_autopct(df['Arrest'].value_counts()),\n",
    "                       figsize=(8, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the successful arrest percentages per year?\n",
    "## Group dataset by year and arrests\n",
    "arrest_per_year = df.groupby('Year')['Arrest'].value_counts().rename('Counts').to_frame()\n",
    "arrest_per_year['Percentage'] = (100 * arrest_per_year / arrest_per_year.groupby(level=0).sum())\n",
    "arrest_per_year.reset_index(level=[1],inplace=True)\n",
    "arrest_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a line plot for percentages of successful arrests over time (2001 to present)\n",
    "line_plot = arrest_per_year[arrest_per_year['Arrest'] == True]['Percentage']\n",
    "\n",
    "## Configure line plot to make visualizing data cleaner\n",
    "labels = line_plot.index.values\n",
    "fig=plt.figure(figsize=(12, 10))\n",
    "plt.title('Percentages of successful arrests from 2011 to 2017')\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Successful Arrest Percentage\")\n",
    "plt.xticks(line_plot.index, line_plot.index.values)\n",
    "\n",
    "line_plot.plot(grid=True, marker='o', color='mediumvioletred')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "chart = sns.countplot(\n",
    "    x='Year',\n",
    "    hue='Arrest',\n",
    "    data=df\n",
    ")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Arrest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,5))\n",
    "chart = sns.countplot(\n",
    "    x='Ward',\n",
    "    hue='Arrest',\n",
    "    data=df\n",
    ")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "chart = sns.countplot(\n",
    "    x='Primary Type',\n",
    "    hue='Arrest',\n",
    "    data=df\n",
    ")\n",
    "chart.set_xticklabels(chart.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Primary Type')['Arrest'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,30))\n",
    "df.groupby([df['Location Description']]).size().sort_values(ascending=True).plot(kind='barh')\n",
    "plt.title('Number of crimes by Location')\n",
    "plt.ylabel('Crime Location')\n",
    "plt.xlabel('Number of crimes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,30))\n",
    "df.groupby([df['Ward']]).size().sort_values(ascending=True).plot(kind='barh')\n",
    "plt.title('Number of crimes by Ward')\n",
    "plt.ylabel('Crime Ward')\n",
    "plt.xlabel('Number of crimes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_graphs=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dates to pandas datetime format and setting the index to be the date will help us a lot later on\n",
    "df_graphs.Date = pd.to_datetime(df_graphs.Date, format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "df_graphs.index = pd.DatetimeIndex(df_graphs.Date)\n",
    "\n",
    "df_graphs.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploration and visualization\n",
    "#Qstn answered:How maany crimes per month between the year 2010-2017\n",
    "plt.figure(figsize=(12,6))\n",
    "df_graphs.resample('M').size().plot()\n",
    "plt.title('Number of crimes per month (2001 - 2019)')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows a clear \"periodic\" pattern in the crimes over many years.\n",
    "\n",
    "I guess this very periodic pattern is an essential part of why crime a very predictable activity!\n",
    "\n",
    "The above chart does show a decresing pattern in the amount of crimes happening from the year 2006-2007 but it's not very clear if all the crimes are decresing.\n",
    "\n",
    "Thus, to find out if all the crimes are decreasing, I have written code that will allow us to see if there is aa decrease in the sum of all crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's see if the sum of all the crime is decresing over the period of time\n",
    "plt.figure(figsize=(12,6))\n",
    "df_graphs.resample('D').size().rolling(365).sum().plot()\n",
    "plt.title('Sum of all crimes from 2001 - 2019')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Number of crimes')\n",
    "plt.show()\n",
    "\n",
    "#below diag shows a decrease in the overall crime rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will take a finer scale to get the visualization right. I decided to look at the rolling sum of crimes . The idea is, for each day, we calculate the sum of crimes. If this rolling sum is decreasing, then we know for sure that crime rates have been decreasing during that year. On the other hand, if the rolling sum stays the same during a given year, then we can conclude that crime rates stayed the same.\n",
    "\n",
    "Thus, from the above chart we can say that the sum of crime has indeed decreased.\n",
    "\n",
    "But now the question that comes to my mind is, Are all the crimes decreasing? Lets find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now let's seperate crime by it's type \n",
    "crimes_count_date = df_graphs.pivot_table('ID', aggfunc=np.size, columns='Primary Type',\n",
    "                                       index=df_graphs.index.date, fill_value=0)\n",
    "crimes_count_date.index = pd.DatetimeIndex(crimes_count_date.index)\n",
    "plo = crimes_count_date.rolling(365).sum().plot(figsize=(12, 30), \n",
    "                                                subplots=True, layout=(-1, 3), \n",
    "                                                sharex=False, sharey=False)\n",
    "\n",
    "#if we were to only believe the previous graph we would have been wrong since some of the crimes have actually \n",
    "#incresed over the period of time\n",
    "#Crimes like Concealed carry license violation,Deceptive practice,Human trafficing etc have show an increasing trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data = session_one_data[(session_one_data['Year'] >= 2011) & (session_one_data['Year'] <= 2017)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data=session_one_data.drop([\"Case Number\", \"Domestic\", \"Beat\", \"Updated On\", \"Arrest\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagb_crime=session_one_data.groupby(\"Primary Type\")[\"Primary Type\"].count()\n",
    "datagb_crime.sort_values(ascending=False, inplace=True)\n",
    "datagb_crime.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_list=datagb_crime.index.values[0:25].tolist()\n",
    "session_one_data=session_one_data[session_one_data[\"Primary Type\"].isin(crime_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data=session_one_data[session_one_data[\"Primary Type\"]!=\"OTHER OFFENSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severe_crime_list=[\"ARSON\", \"ASSAULT\", \"BATTERY\", \"CRIM SEXUAL ASSAULT\", \"CRIMINAL DAMAGE\", \"CRIMINAL TRESPASS\", \"HOMICIDE\", \"ROBBERY\"]\n",
    "session_one_data[\"severe\"]=np.where(session_one_data['Primary Type'].isin(severe_crime_list), 1, 0)\n",
    "session_one_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(session_one_data.groupby(\"Location Description\")[\"Location Description\"].count().index.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagb_location=session_one_data.groupby(\"Location Description\")[\"Location Description\"].count()\n",
    "datagb_location.sort_values(ascending=False, inplace=True)\n",
    "datagb_location.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_list=datagb_location.index.values[0:25].tolist()\n",
    "session_one_data=session_one_data[session_one_data[\"Location Description\"].isin(location_list)]\n",
    "session_one_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session_one_data.groupby(\"District\")[\"District\"].count())\n",
    "print(session_one_data.groupby(\"Community Area\")[\"Community Area\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagb_destrict=session_one_data.groupby(\"District\")[\"District\"].count()\n",
    "district_list=datagb_destrict.index.values[0:22].tolist()\n",
    "session_one_data=session_one_data[session_one_data[\"District\"].isin(district_list)]\n",
    "session_one_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_one_data[\"District\"]='D'+session_one_data['District'].astype(str)\n",
    "session_one_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a dummy variable for district and primary type of crime\n",
    "dummydf=pd.get_dummies(session_one_data,columns=[\"Primary Type\",\"District\"])\n",
    "#we will just make a copy here in case we need to use it in the future\n",
    "dummydf=dummydf.join(session_one_data[[\"District\",\"Primary Type\"]])\n",
    "print(dummydf.shape)\n",
    "dummydf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedataf=dummydf.copy()\n",
    "from datetime import datetime\n",
    "format = '%m/%d/%Y %I:%M:%S %p'\n",
    "dummydf[\"time_24hour\"]=dummydf.Date.apply(lambda row: datetime.strptime(row, format).strftime(\"%H:%M\"))\n",
    "dummydf[\"Timeblock\"]=dummydf.Date.apply(lambda row: str(3*int(int(datetime.strptime(row, format).strftime(\"%H\"))/3)))\n",
    "dummydf['Date_no_time']=dummydf.Date.apply(lambda row: datetime.strptime(row, format).strftime(\"%Y%m%d\"))\n",
    "dummydf[\"Weekday\"]=dummydf.Date.apply(lambda row: datetime.strptime(row, format).strftime(\"%A\"))\n",
    "dummydf=pd.get_dummies(dummydf,columns=[\"Timeblock\",\"Weekday\"])\n",
    "dummydf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummydf = dummydf.dropna()\n",
    "dummydf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on Police Station Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago=dummydf\n",
    "police_df=pd.read_csv(\"dataset/police-stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_lati = np.array(chicago[\"Latitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_longi = np.array(chicago[\"Longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_lat_long = zip(chicago_lati, chicago_longi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_lat_long_list = list(chicago_lat_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_lat_long_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_array = [(41.8583725929, -87.627356171), (41.8018110912, -87.6305601801), \n",
    "                  (41.7664308925, -87.6057478606), (41.7079332906, -87.5683491228),\n",
    "                  (41.6927233639, -87.6045058667),\n",
    "                 (41.7521368378, -87.6442289066), (41.7796315359, -87.6608870173),\n",
    "                  (41.778987189, -87.7088638153),(41.8373944311, -87.6464077068),\n",
    "                  (41.8566845327, -87.708381958),(41.8735822883, -87.705488126),\n",
    "                  (41.8629766244, -87.6569725149),(41.9211033246, -87.6974518223),\n",
    "                  (41.8800834614, -87.768199889),(41.9740944511, -87.7661488432),\n",
    "                  (41.9660534171, -87.728114561),(41.9032416531, -87.6433521393),\n",
    "                  (41.9474004564, -87.651512018),(41.9795495131, -87.6928445094),\n",
    "                  (41.6914347795, -87.6685203937), (41.9997634842, -87.6713242922),\n",
    "                  (41.9186088912, -87.765574479)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_array = np.array (stations_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago[\"Location\"] = chicago_lat_long_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating distance between particular crime location and its closest police station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Harvesine Formula to calculate the closest police station to each of the crime scenes in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from math import radians, sin, cos, asin, sqrt, pi, atan2\n",
    "import itertools\n",
    "from timeit import Timer\n",
    "\n",
    "distance = []\n",
    "earth_radius_miles = 3956.0\n",
    "\n",
    "x = chicago[\"Location\"]\n",
    "y = station_array \n",
    "\n",
    "def get_shortest_in(needle, haystack):\n",
    "    # needle is a single (lat,long) tuple. haystack is a numpy array to find the point in that has the shortest distance to needle  \n",
    "    \n",
    "    dlat = np.radians(haystack[:,0]) - radians(needle[0])\n",
    "    dlon = np.radians(haystack[:,1]) - radians(needle[1])\n",
    "    a = np.square(np.sin(dlat/2.0)) + cos(radians(needle[0])) * np.cos(np.radians(haystack[:,0])) * np.square(np.sin(dlon/2.0))\n",
    "    great_circle_distance = 2 * np.arcsin(np.minimum(np.sqrt(a), np.repeat(1, len(a))))\n",
    "    d = earth_radius_miles * great_circle_distance\n",
    "    return np.min(d)\n",
    "\n",
    "\n",
    "\n",
    "def donumpy():\n",
    "    get_shortest_in(x, y)\n",
    "    \n",
    "for i in x:\n",
    "    distance.append(get_shortest_in(i, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago[\"closest_station\"] = distance\n",
    "chicago.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf = chicago\n",
    "inc_edu_age = pd.read_csv(\"dataset/Census_Data_-_Selected_socioeconomic_indicators_in_Chicago__2008___2012.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_edu_age.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf[\"Income\"]=smalldf[\"Community Area\"].apply(lambda row: inc_edu_age.iloc[int(row)-1].iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf['HARDSHIP INDEX']=smalldf[\"Community Area\"].apply(lambda row: inc_edu_age.iloc[int(row)-1].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf['Under18_over64']=smalldf[\"Community Area\"].apply(lambda row: inc_edu_age.iloc[int(row)-1].iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf['Unemployed']=smalldf[\"Community Area\"].apply(lambda row: inc_edu_age.iloc[int(row)-1].iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf['House_below_poverty']=smalldf[\"Community Area\"].apply(lambda row: inc_edu_age.iloc[int(row)-1].iloc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf.head()\n",
    "print(smalldf.shape)\n",
    "smalldf=smalldf[smalldf[\"Location Description\"]!=\"OTHER\"]\n",
    "smalldf=pd.get_dummies(smalldf,columns=[\"Location Description\"])\n",
    "print(smalldf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=smalldf.sample(80000)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\"HARDSHIP INDEX\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\"ID\",\"Date\",\"Block\",\"IUCR\",\"Description\",\"Ward\",\"Community Area\",\"FBI Code\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\"X Coordinate\",\"Y Coordinate\",\"Year\",\"Location\",\"District\",\"Primary Type\",\"time_24hour\",\"Date_no_time\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(data.columns.values)\n",
    "droplist=[]\n",
    "for i in a:\n",
    "    if i.startswith(\"Primary Type\"):\n",
    "        droplist.append(i)\n",
    "data=data.drop(droplist, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itrain, itest = train_test_split(range(data.shape[0]), train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.ones(data.shape[0], dtype='int')\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask=(mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan_to_num(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have a list of continuous features, or in other words standardizable variables\n",
    "STANDARDIZABLE=[\"Latitude\", \"Longitude\",\"closest_station\",\"House_below_poverty\",\"Unemployed\",\"Under18_over64\",\"Income\"]\n",
    "\n",
    "#Also create a list for indicator variable. We can do this by excluding the above continuous features from total features. \n",
    "INDICATOR=list(data.columns)\n",
    "#We need to remove the response variable from our total list of features\n",
    "INDICATOR.remove(u'severe')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(STANDARDIZABLE), len(INDICATOR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STANDARDIZABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Standardize training set\n",
    "data.loc[mask,STANDARDIZABLE]=StandardScaler().fit_transform(data.loc[mask,STANDARDIZABLE])\n",
    "#Standardize test set\n",
    "data.loc[~mask,STANDARDIZABLE]=StandardScaler().fit_transform(data.loc[~mask,STANDARDIZABLE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(24,36))\n",
    "pos=data[data[\"severe\"]==1]\n",
    "neg=data[data[\"severe\"]==0]\n",
    "for k in range (7):\n",
    "    ax=fig.add_subplot(5,3,k+1)    \n",
    "    sns.kdeplot(pos[STANDARDIZABLE[k]],color=\"red\",label=\"Severe\")\n",
    "    sns.kdeplot(neg[STANDARDIZABLE[k]],color=\"blue\",label=\"Not severe\")\n",
    "    ax.set_title(STANDARDIZABLE[k])\n",
    "    ax.set_xlabel(\"Normalized Z-score\")\n",
    "    ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following command just plot those continuous features with significant effects (for presentation purpose)\n",
    "plotlist=[\"Latitude\",\"House_below_poverty\",\"Unemployed\",\"Under18_over64\",\"Income\"]\n",
    "plotlist_title=[\"Latitude\",\"Proportion of House Below Poverty\",\"Proportion Unemployed\", \"Proportion with Age Under 18 Over 64\", \"Income per Capita\"]\n",
    "fig=plt.figure(figsize=(24,18))\n",
    "for k in range (5):\n",
    "    ax=fig.add_subplot(2,3,k+1)    \n",
    "    sns.kdeplot(pos[plotlist[k]],color=\"red\",label=\"Severe\")\n",
    "    sns.kdeplot(neg[plotlist[k]],color=\"blue\",label=\"Not severe\")\n",
    "    ax.set_title(plotlist_title[k])\n",
    "    ax.set_xlabel(\"Normalized Z-score\")\n",
    "    ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30,144))\n",
    "pos=data[data['severe']==1]\n",
    "neg=data[data['severe']==0]\n",
    "for k in range (68):\n",
    "    ax=fig.add_subplot(17,4,k+1)\n",
    "    ax.hist((pos[INDICATOR[k]],neg[INDICATOR[k]]),stacked=True,color=(\"red\",\"blue\"),range=[0,1])\n",
    "    ax.set_title(INDICATOR[k])\n",
    "    ax.legend((\"Severe\",\"Not severe\"),loc=\"upper center\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Crime Counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is to plot those indicator variables that will be good predictor for whether a crime will be severe or not.\n",
    "fig=plt.figure(figsize=(26,18))\n",
    "plotfeature=[\"District_D11.0\",\"Timeblock_3\",\"Timeblock_9\",\"Timeblock_12\",\"Location Description_APARTMENT\",\"Location Description_STREET\"]\n",
    "plotfeature_title=[\"Police District 11\",\"Time: 3am-6am\",\"Time: 9am-12pm\", \"Time: 12pm to 3pm\",\"Location: apartment\",\"Location: street\"]\n",
    "for k in range (6):\n",
    "    ax=fig.add_subplot(2,3,k+1)\n",
    "    ax.hist((pos[plotfeature[k]],neg[plotfeature[k]]),stacked=True,color=(\"red\",\"blue\"),range=[0,1])\n",
    "    ax.set_title(plotfeature_title[k])\n",
    "    ax.legend((\"Severe\",\"Not severe\"),loc=\"upper center\")\n",
    "    ax.set_xlabel(\"Value\")\n",
    "    ax.set_ylabel(\"Crime Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0 - Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=data[data['severe']==1]\n",
    "neg=data[data['severe']==0]\n",
    "percent_severe=float(len(pos))/len(data)\n",
    "percent_non_severe=float(len(neg))/len(data)\n",
    "print (percent_severe, percent_non_severe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make a dictionary storing confusion matrix for all the algorithms, so that we can have some comparison\n",
    "confusion_dict={}\n",
    "confusion_dict[\"Baseline_model\"]=np.asarray([[len(neg),0],[len(pos),0]])\n",
    "#Also create a dictionary to store all the models\n",
    "model_dict={}\n",
    "#The following dict will store the accuracy for training set\n",
    "accuracy_dict={}\n",
    "#The following dict will store the accuracy for test set\n",
    "accuracy_dict1={}\n",
    "train_not_severe_percent=1-float(sum(data[\"severe\"].values[mask]))/len(data[\"severe\"].values[mask])\n",
    "test_not_severe_percent=1-float(sum(data[\"severe\"].values[~mask]))/len(data[\"severe\"].values[~mask])\n",
    "print (train_not_severe_percent, test_not_severe_percent) \n",
    "accuracy_dict[\"Baseline_model\"]=train_not_severe_percent\n",
    "accuracy_dict1[\"Baseline_model\"]=test_not_severe_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1 - Logistic regression with Lasso-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got the X and y for traning set and test set\n",
    "total_features=STANDARDIZABLE+INDICATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: hw3 do_classify function\n",
    "#Slightly modify the hw3 function, but overall very similar\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def do_classify(clf, parameters, indf, featurenames, targetname, target1val, mask, score_func=None, n_folds=5):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "    y=(indf[targetname].values==target1val)*1\n",
    "    Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print (\"Training accuracy: %0.2f\" % (training_accuracy))\n",
    "    print (\"Test accuracy:     %0.2f\" % (test_accuracy))\n",
    "    confmatrix=confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print (confmatrix)\n",
    "    print (clf)\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest, confmatrix, training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: hw3 cv_optimize function\n",
    "#we will use five fold validation by default\n",
    "#This function is largely the same as the one in our hw\n",
    "def cv_optimize(clf, parameters, X, y, n_folds=5, score_func=None):\n",
    "    if score_func:\n",
    "        gs=GridSearchCV(clf, param_grid=parameters, cv=n_folds, scoring=score_func)\n",
    "    else:\n",
    "        gs=GridSearchCV(clf, param_grid=parameters, cv=n_folds)\n",
    "    gs.fit(X, y)\n",
    "    best = gs.best_estimator_\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clflog = LogisticRegression(penalty=\"none\")\n",
    "clflog, Xtrain, ytrain, Xtest, ytest, confclflog, training_accuracy, test_accuracy=do_classify(clflog, {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 20.0, 40.0, 70.0, 100.0]}, data, total_features, u'severe', 1, mask=mask)\n",
    "confusion_dict[\"Logistic\"]=confclflog\n",
    "model_dict[\"Logistic\"]=clflog\n",
    "accuracy_dict[\"Logistic\"]=training_accuracy\n",
    "accuracy_dict1[\"Logistic\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in addition to l2 (lasso) regularization, we also tried l2 regularization (the default mode). It works equally well and seems to run faster.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clflog2 = LogisticRegression(penalty=\"l2\")\n",
    "clflog2, Xtrain, ytrain, Xtest, ytest, confclflog2, training_accuracy, test_accuracy=do_classify(clflog2, {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 20.0, 40.0, 70.0, 100.0]}, data, total_features, u'severe', 1, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2 - Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clfsvm=LinearSVC(loss=\"hinge\")\n",
    "clfsvm, Xtrain, ytrain, Xtest, ytest, confclfsvm, training_accuracy, test_accuracy= do_classify(clfsvm, {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 50, 100.0]}, data, total_features, u'severe',1, mask=mask)\n",
    "confusion_dict[\"svm\"]=confclfsvm\n",
    "model_dict[\"svm\"]=clfsvm\n",
    "accuracy_dict[\"svm\"]=training_accuracy\n",
    "accuracy_dict1[\"svm\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3 - Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clfdt=DecisionTreeClassifier()\n",
    "clfdt, Xtrain, ytrain, Xtest, ytest, confclfdt, training_accuracy, test_accuracy = do_classify(clfdt, {\"max_depth\":np.arange(1,20,2)}, data, total_features, u'severe',1, mask=mask)\n",
    "confusion_dict[\"decision tree\"]=confclfdt\n",
    "model_dict[\"decision tree\"]=clfdt\n",
    "accuracy_dict[\"decision tree\"]=training_accuracy\n",
    "accuracy_dict1[\"decision tree\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 - Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clfgnb = GaussianNB()\n",
    "clfgnb, Xtrain, ytrain, Xtest, ytest, confgnb, training_accuracy, test_accuracy=do_classify(clfgnb, None, data, total_features, u'severe',1, mask=mask)\n",
    "confusion_dict[\"Naive Bayes\"]=confgnb\n",
    "model_dict[\"Naive Bayes\"]=clfgnb\n",
    "accuracy_dict[\"Naive Bayes\"]=training_accuracy\n",
    "accuracy_dict1[\"Naive Bayes\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5 - Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randf=RandomForestClassifier()\n",
    "clfrdf, Xtrain, ytrain, Xtest, ytest, confrdf, training_accuracy, test_accuracy=do_classify(randf, {\"n_estimators\":[10, 20, 30, 40, 100]}, data, total_features, u'severe',1, mask=mask)\n",
    "confusion_dict[\"Random forest\"]=confrdf\n",
    "model_dict[\"Random forest\"]=clfrdf\n",
    "accuracy_dict[\"Random forest\"]=training_accuracy\n",
    "accuracy_dict1[\"Random forest\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6 - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier()\n",
    "neigh, Xtrain1, ytrain1, Xtest1, ytest1, confknn, training_accuracy, test_accuracy=do_classify(neigh, {\"n_neighbors\":[5, 10, 20, 40]}, data, total_features, u'severe',1, mask=mask)\n",
    "confusion_dict[\"KNN\"]=confknn\n",
    "model_dict[\"KNN\"]=neigh\n",
    "accuracy_dict[\"KNN\"]=training_accuracy\n",
    "accuracy_dict1[\"KNN\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare training and testing accuracy\n",
    "pd.Series(accuracy_dict).plot(kind=\"bar\",title=\"Training accuracy\",width=0.8,color=\"grey\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare training and testing accuracy\n",
    "pd.Series(accuracy_dict1).plot(kind=\"bar\",title=\"Test accuracy\",width=0.8, color=\"grey\",fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify important factors determining whether a crime is severe or not based on coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero_lasso(clf):\n",
    "    featuremask=(clf.coef_ !=0.0)[0]\n",
    "    return pd.DataFrame(dict(feature=total_features, coef=clf.coef_[0], abscoef=np.abs(clf.coef_[0])))[featuremask].sort_values('abscoef', ascending=False)\n",
    "lasso_importances=nonzero_lasso(clflog)\n",
    "lasso_importances.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification into crime types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=smalldf.sample(80000)\n",
    "data2=data2.drop([\"ID\",\"Date\",\"Block\",\"IUCR\",\"Description\",\"Ward\",\"Community Area\",\"FBI Code\",\"severe\"], axis=1)\n",
    "data2=data2.drop([\"X Coordinate\",\"Y Coordinate\",\"Year\",\"Location\",\"District\",\"time_24hour\",\"Date_no_time\"], axis=1)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.nan_to_num(data2)\n",
    "#data2 = data2.fillna(data2.mean())\n",
    "#data2 = data2.dropna()\n",
    "\n",
    "#data2.isnull().values.any()\n",
    "#data2.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data2.shape)\n",
    "data2[\"Crime_interested\"]=data2[\"Primary Type_THEFT\"]+data2[\"Primary Type_BATTERY\"]+data2[\"Primary Type_NARCOTICS\"]+data2[\"Primary Type_CRIMINAL DAMAGE\"]\n",
    "#only maintain the four specific crime types we are interested in classifying\n",
    "data2=data2[data2[\"Crime_interested\"]==1]\n",
    "print(data2.shape)\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_integer(row):\n",
    "    if row==\"THEFT\":\n",
    "        return int(0)\n",
    "    elif row==\"BATTERY\":\n",
    "        return int(1)\n",
    "    elif row==\"CRIMINAL DAMAGE\":\n",
    "        return int(2)\n",
    "    elif row==\"NARCOTICS\":\n",
    "        return int(3)\n",
    "\n",
    "#Assign each of the crime type an integer identifier. We will input these integer identifier directly for the algorithms below.\n",
    "data2[\"category\"]=data2[\"Primary Type\"].apply(get_categorical_integer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Previously we dropped rows that are not in the four types, which mess up the integer index of the dataframe\n",
    "#here we reset the integer index. This is required for the mask to work.\n",
    "#data2 = data2.reset_index(drop=True)\n",
    "data2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_classify2(clf, parameters, indf, featurenames,targetname, mask, score_func=None, n_folds=5):\n",
    "    subdf=indf[featurenames]\n",
    "    X=subdf.values\n",
    "#y will be an array with integer values for different categories (0, 1, 2, 3...)\n",
    "    y=indf[targetname]\n",
    "    Xtrain, Xtest, ytrain, ytest = X[mask], X[~mask], y[mask], y[~mask]\n",
    "    if parameters:\n",
    "        clf = cv_optimize(clf, parameters, Xtrain, ytrain, n_folds=n_folds, score_func=score_func)\n",
    "    clf=clf.fit(Xtrain, ytrain)\n",
    "    training_accuracy = clf.score(Xtrain, ytrain)\n",
    "    test_accuracy = clf.score(Xtest, ytest)\n",
    "    print (\"Training accuracy: %0.2f\" % (training_accuracy))\n",
    "    print( \"Test accuracy:     %0.2f\" % (test_accuracy))\n",
    "    confmatrix=confusion_matrix(ytest, clf.predict(Xtest))\n",
    "    print (confmatrix)\n",
    "    print (clf)\n",
    "    return clf, Xtrain, ytrain, Xtest, ytest, training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask=np.ones(data2.shape[0], dtype='int')\n",
    "itrain, itest = train_test_split(range(data2.shape[0]), train_size=0.7)\n",
    "mask[itrain]=1\n",
    "mask[itest]=0\n",
    "mask=(mask==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import StandardScaler\n",
    "#Standardize training set\n",
    "data2.loc[mask,STANDARDIZABLE]=StandardScaler().fit_transform(data2.loc[mask,STANDARDIZABLE])\n",
    "#Standardize test set\n",
    "data2.loc[~mask,STANDARDIZABLE]=StandardScaler().fit_transform(data2.loc[~mask,STANDARDIZABLE])\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 0: Base-line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(float(len(data2[data2[\"category\"]==0]))/len(data2))\n",
    "train_theft_percent=float(sum(data2[\"Primary Type_THEFT\"].values[mask]))/len(data2[\"Primary Type_THEFT\"].values[mask])\n",
    "test_theft_percent=float(sum(data2[\"Primary Type_THEFT\"].values[~mask]))/len(data2[\"Primary Type_THEFT\"].values[~mask])\n",
    "\n",
    "print(train_theft_percent)\n",
    "print(test_theft_percent)\n",
    "\n",
    "accuracy_multi_train={}\n",
    "accuracy_multi_test={}\n",
    "accuracy_multi_train[\"Baseline model\"]=train_theft_percent\n",
    "accuracy_multi_test[\"Baseline model\"]=test_theft_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1: Decision tree for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clfdt_multi=DecisionTreeClassifier()\n",
    "clfdt_multi, Xtrain, ytrain, Xtest, ytest, training_accuracy, test_accuracy=do_classify2(clfdt_multi, {\"max_depth\":np.arange(1,20,2)}, data2, total_features, \"category\", mask=mask)\n",
    "accuracy_multi_train[\"Decision tree multiclass\"]=training_accuracy\n",
    "accuracy_multi_test[\"Decision tree multiclass\"]=test_accuracy\n",
    "\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# clfdt=DecisionTreeClassifier()\n",
    "# clfdt, Xtrain, ytrain, Xtest, ytest, confclfdt, training_accuracy, test_accuracy = do_classify(clfdt, {\"max_depth\":np.arange(1,20,2)}, data, total_features, u'severe',1, mask=mask)\n",
    "# confusion_dict[\"decision tree\"]=confclfdt\n",
    "# model_dict[\"decision tree\"]=clfdt\n",
    "# accuracy_dict[\"decision tree\"]=training_accuracy\n",
    "# accuracy_dict1[\"decision tree\"]=test_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Random forest for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randfmulti=RandomForestClassifier()\n",
    "randfmulti, Xtrain, ytrain, Xtest, ytest, training_accuracy, test_accuracy=do_classify2(randfmulti, {\"n_estimators\":[10, 20, 30, 40, 100]}, data2, total_features, \"category\", mask=mask)\n",
    "accuracy_multi_train[\"Random forest multiclass\"]=training_accuracy\n",
    "accuracy_multi_test[\"Random forest multiclass\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Logistic regression for multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clflogmulti=LogisticRegression(penalty=\"l2\",multi_class='multinomial',solver=\"newton-cg\",max_iter=100)\n",
    "clflogmulti, Xtrain, ytrain, Xtest, ytest, training_accuracy, test_accuracy=do_classify2(clflogmulti, {\"C\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]}, data2, total_features, 'category', mask=mask)\n",
    "accuracy_multi_train[\"Logistic - newton cg\"]=training_accuracy\n",
    "accuracy_multi_test[\"Logistic - newton cg\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clflogmulti2 = LogisticRegression(penalty=\"l2\",multi_class='multinomial',solver=\"lbfgs\",max_iter=400)\n",
    "clflogmulti2, Xtrain, ytrain, Xtest, ytest, training_accuracy, test_accuracy=do_classify2(clflogmulti2, {\"C\": [0.01, 0.1, 1.0, 10.0]}, data2, total_features, 'category', mask=mask)\n",
    "accuracy_multi_train[\"Logistic - lbfgs\"]=training_accuracy\n",
    "accuracy_multi_test[\"Logistic - lbfgs\"]=test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare training and testing accuracy\n",
    "pd.Series(accuracy_multi_train).plot(kind=\"bar\",title=\"Training accuracy\",width=0.8,color=\"grey\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare training and testing accuracy\n",
    "pd.Series(accuracy_multi_test).plot(kind=\"bar\",title=\"Test accuracy\",width=0.8,color=\"grey\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is the accuracy for two-type prediction for a baseline model?\n",
    "total_predict=len(ytest)\n",
    "correct_predict=0\n",
    "baseline_accuracy=0\n",
    "for i in range(0,total_predict):\n",
    "    if ((ytest.iloc[i]==0) | (ytest.iloc[i]==1)):\n",
    "        correct_predict += 1\n",
    "    if ytest.iloc[i]==0:\n",
    "        baseline_accuracy += 1\n",
    "print(float(correct_predict)/total_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: http://stackoverflow.com/questions/6910641/how-to-get-indices-of-n-maximum-values-in-a-numpy-array\n",
    "def accuracy_two_type(est, Xtest, ytest):\n",
    "    probs=est.predict_proba(Xtest)\n",
    "    correct_predict=0\n",
    "    total_predict=len(ytest)\n",
    "#For each line (a prediction with four probabilities for the four categories), we obtain the two most likely classes\n",
    "#We checked whether the actual observation is in one of the two predictions\n",
    "#If so, we call the result for this record of crime accurate\n",
    "    for i in range(0,total_predict):\n",
    "#This will return the column index of the two largest probabilities, which directly corresponds to type of crime\n",
    "        ind=np.argpartition(probs[i,],-2)[-2:]\n",
    "        if ytest.iloc[i] in ind:\n",
    "            correct_predict+=1\n",
    "        i+=1\n",
    "    return float(correct_predict)/total_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_two_type(clflogmulti, Xtest, ytest))\n",
    "print(accuracy_two_type(clflogmulti2, Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_importances_multi=nonzero_lasso(clflogmulti)\n",
    "lasso_importances_multi.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimedata =chicago\n",
    "print(crimedata.shape)\n",
    "crimedata.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting crime rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will plot the occurence rates of the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Crime Type\n",
    "2) Scene of Crime\n",
    "3) Time of Crime\n",
    "4) Day of Crime\n",
    "5) Month of Crime\n",
    "6) Average Temperature of Crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occurrence rates of the various types of crime\n",
    "crimetypegb=crimedata.groupby([\"Primary Type\"])[\"Primary Type\"].count()/len(crimedata)*100\n",
    "crimetypegb.sort_values(ascending=False, inplace=True)\n",
    "print(crimetypegb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimetypegb.plot(kind='bar',title=\"Type of Crime\")\n",
    "plt.ylabel('Occurrence rate (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationgb=crimedata.groupby(['Location Description'])[\"Location Description\"].count()/len(crimedata)*100\n",
    "locationgb.sort_values(ascending=False, inplace=True)\n",
    "print(locationgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locationgb.plot(kind='bar',title=\"Scene of Crime\")\n",
    "plt.ylabel('Occurrence rate (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "format = '%m/%d/%Y %I:%M:%S %p'\n",
    "crimedata[\"time_hour\"]=crimedata.Date.apply(lambda row: datetime.strptime(row, format).strftime(\"%H\"))\n",
    "crimedata[\"month\"]=crimedata.Date.apply(lambda row: datetime.strptime(row, format).strftime(\"%m\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegb=crimedata.groupby(['time_hour'])[\"time_hour\"].count()/len(crimedata)*100\n",
    "print(timegb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timegb.plot(kind='bar',title=\"Hour Crime occurred\")\n",
    "plt.ylabel('Occurrence rate (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday=crimedata[[\"Weekday_Monday\",\"Weekday_Tuesday\",\"Weekday_Wednesday\",\"Weekday_Thursday\",\"Weekday_Friday\",\"Weekday_Saturday\",\"Weekday_Sunday\"]].sum()/len(crimedata)*100\n",
    "print(weekday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday.plot(kind='bar',title=\"Day Crime occurred\") #color='bgyrc'\n",
    "plt.ylabel('Occurrence rate (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_group=crimedata.groupby(['month'])['month'].count()/len(crimedata)*100\n",
    "print(month_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_group.plot(kind='bar',title=\"Month Crime occurred\")\n",
    "plt.ylabel('Occurrence rate (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we focus on only the 4 major types of crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topfour(row):\n",
    "    keep = [\"THEFT\", \"BATTERY\", \"NARCOTICS\", \"CRIMINAL DAMAGE\"]\n",
    "    if row not in keep:\n",
    "        return \"OTHERS\"\n",
    "    else:\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimedata[\"New_Type\"] = crimedata[\"Primary Type\"].apply(topfour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we write a function to take in a column name, title, and return a plot that displays the percentage, per column\n",
    "# with the normalized types of crimes for each feature in the selected column\n",
    "def plotsplit(cnam, title):\n",
    "    datasplit = crimedata.groupby([cnam, \"New_Type\"])[cnam].count().unstack()\n",
    "    # Convert everything to percentage for normalization, so we can compare!\n",
    "    datasplit= datasplit.apply(lambda c: c / c.sum() * 100, axis=1)\n",
    "    # Reorder columns\n",
    "    datasplit = datasplit[['OTHERS', 'CRIMINAL DAMAGE', 'NARCOTICS', 'BATTERY', 'THEFT']]\n",
    "    datasplit.plot(kind = \"bar\", stacked = True, title = title)\n",
    "    plt.ylabel('Fraction of Crime Type (%)')\n",
    "    # Anchoring legend from http://stackoverflow.com/questions/4700614/how-to-put-the-legend-out-of-the-plot\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1), ncol=3, fancybox=True, shadow=True)\n",
    "    plt.ylim([0,120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pltd\n",
    "\n",
    "pltd.rcParams[\"figure.figsize\"] = (12,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotsplit(\"Location Description\", \"Normalized Crime Types by Location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crime type per hour of day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotsplit(\"time_hour\", \"Normalized Crime Types by Time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crimes by day: First we need to un-get_dummies the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to turn Mon, Tue, Wed, Thur, Fri, Sat, Sun into 1 - 7\n",
    "crimedata[\"Weekday_Tuesday\"] = (crimedata[\"Weekday_Tuesday\"].apply(lambda x: x+1 if x > 0 else 0))\n",
    "crimedata[\"Weekday_Wednesday\"] = (crimedata[\"Weekday_Wednesday\"].apply(lambda x: x+2 if x > 0 else 0))\n",
    "crimedata[\"Weekday_Thursday\"] = (crimedata[\"Weekday_Thursday\"].apply(lambda x: x+3 if x > 0 else 0))\n",
    "crimedata[\"Weekday_Friday\"] = (crimedata[\"Weekday_Friday\"].apply(lambda x: x+4 if x > 0 else 0))\n",
    "crimedata[\"Weekday_Saturday\"] = (crimedata[\"Weekday_Saturday\"].apply(lambda x: x+5 if x > 0 else 0))\n",
    "crimedata[\"Weekday_Sunday\"] = (crimedata[\"Weekday_Sunday\"].apply(lambda x: x+6 if x > 0 else 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimedata[\"Num_Day\"] = crimedata[\"Weekday_Monday\"] + crimedata[\"Weekday_Tuesday\"] + crimedata[\"Weekday_Wednesday\"] + crimedata[\"Weekday_Thursday\"] + crimedata[\"Weekday_Friday\"] + crimedata[\"Weekday_Saturday\"] + crimedata[\"Weekday_Sunday\"]\n",
    "crimedata[\"Num_Day\"] = crimedata[\"Num_Day\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plotsplit(\"Num_Day\", \"Normalized Crime Types by Day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotsplit(\"month\", \"Normalized Crime Types by Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now lets make a map to visualize the crime locations and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(m, height=300):\n",
    "    \"\"\"Takes a folium instance and embed HTML.\"\"\"\n",
    "    m._build_map()\n",
    "    srcdoc = m.HTML.replace('\"', '&quot;')\n",
    "    embed = HTML('<iframe srcdoc=\"{0}\" '\n",
    "                 'style=\"width: 100%; height: {1}px; '\n",
    "                 'border: none\"></iframe>'.format(srcdoc, height))\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start an instance of the Chicago map like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = folium.Map(location=[41.8369, -87.6847], zoom_start=10)\n",
    "folium.Marker([41.8369, -87.6847]).add_to(map) \n",
    "#display(map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code is fine but little bit error in object creation\n",
    "from folium.plugins import MarkerCluster\n",
    "marker_cluster = MarkerCluster().add_to(map)\n",
    "##mapping the main crime types to the map \n",
    "types = ['OTHERS', 'CRIMINAL DAMAGE', 'NARCOTICS', 'BATTERY', 'THEFT']\n",
    "for i in types:\n",
    "    typedata=crimedata[crimedata[\"New_Type\"]==i]\n",
    "    map = folium.Map(location=[41.8369, -87.6847], zoom_start=10)\n",
    "    #add a marker for every record in the filtered data, use a clustered view\n",
    "    for each in typedata[0:len(typedata)].iterrows():\n",
    "        folium.Marker(\n",
    "            location = [each[1]['Latitude'],each[1]['Longitude']]).add_to(marker_cluster)\n",
    "    #display(map)\n",
    "    map.create_map(path=i + 'map.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
